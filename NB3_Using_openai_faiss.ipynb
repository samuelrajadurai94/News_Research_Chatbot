{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import time\n",
    "import langchain\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat token from hugging face website and use it.\n",
    "#load hugging face token key key\n",
    "os.environ['HF_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for using openAI model:\n",
    "# Initialise LLM with required params\n",
    "llm = OpenAI(temperature=0.9, max_tokens=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders = UnstructuredURLLoader(urls=[\n",
    "                  \"https://www.coursera.org/articles/software-engineer\",\n",
    "                  \"https://www.coursera.org/articles/computer-information-systems\",\n",
    "                  'https://www.coursera.org/in/articles/machine-learning-models'])\n",
    "data = loaders.load() \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loaders.load() \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Split data to create chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RecursiveCharacterTextSplitter:\n",
    "The RecursiveCharacterTextSplitter takes a large text and splits it based on a specified chunk size. It does this by using a set of characters. The default characters provided to it are [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
    "\n",
    "It takes in the large text then tries to split it by the first character \\n\\n. If the first split by \\n\\n is still large then it moves to the next character which is \\n and tries to split by it. If it is still larger than our specified chunk size it moves to the next character in the set until we get a split that is less than our specified chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,   # number of characters for each chunks (note:'not words')\n",
    "    chunk_overlap=200   # number of characters to overlap.\n",
    ")\n",
    "\n",
    "# As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Popular Cybersecurity Certifications\\n\\nPopular Data Analytics Certifications\\n\\nPopular IT Certifications\\n\\nPopular Machine Learning Certifications\\n\\nPopular SQL Certifications\\n\\nGenAI for Software Developers\\n\\nGenAI for Data Professionals\\n\\nCareer Insights & Advice Hub\\n\\nCoursera\\n\\nAbout\\n\\nWhat We Offer\\n\\nLeadership\\n\\nCareers\\n\\nCatalog\\n\\nCoursera Plus\\n\\nProfessional Certificates\\n\\nMasterTrack® Certificates\\n\\nDegrees\\n\\nFor Enterprise\\n\\nFor Government\\n\\nFor Campus\\n\\nBecome a Partner\\n\\nSocial Impact\\n\\nFree Courses\\n\\nECTS Credit Recommendations\\n\\nCommunity\\n\\nLearners\\n\\nPartners\\n\\nBeta Testers\\n\\nBlog\\n\\nThe Coursera Podcast\\n\\nTech Blog\\n\\nTeaching Center\\n\\nMore\\n\\nPress\\n\\nInvestors\\n\\nTerms\\n\\nPrivacy\\n\\nHelp\\n\\nAccessibility\\n\\nContact\\n\\nArticles\\n\\nDirectory\\n\\nAffiliates\\n\\nModern Slavery Statement\\n\\nManage Cookie Preferences\\n\\nLearn Anywhere\\n\\n© 2024 Coursera Inc. All rights reserved.' metadata={'source': 'https://www.coursera.org/in/articles/machine-learning-models'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Create embeddings for these chunks and save them to FAISS index\n",
    " since we didnt have paid account in openAI, we cant able to use openAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings of the chunks using openAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Pass the documents and embeddings inorder to create FAISS vector index\n",
    "vectorindex_openai = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, OpenAI Embeddings are expensive. Let’s explore some best performing open source embedding models.\n",
    "\n",
    "BGE Model( BAAI(Beijing Academy of Artificial Intelligence) General Embeddings) Model\n",
    "\n",
    "BGE models on HuggingFaceare one of the best open source embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\":'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings':True}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a embedding model, we are ready to vectorize all our documents and store them in a vector store to construct a retrieval system. With specifically designed searching algorithms, a retrieval system can do similarity searching efficiently to retrieve relevant documents.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions (nearest-neighbor search implementations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create embeddings and save it to FAISS index\n",
    "    #embeddings = OpenAIEmbeddings()\n",
    "vectordb_bge = FAISS.from_documents(docs, hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"faiss_store_bge.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(vectordb_bge, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectordb_bge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Samuel_Learnings\\NewsResearch_chatbot_Langchain\\sam_vs\\lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "with open(\"faiss_store_bge.pkl\", \"rb\") as f:\n",
    "            vectordb_bge = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a retriever interface using vector store, we’ll use it later to construct Q & A chain using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use similarity searching algorithm and return top 3 relevant document.\n",
    "retriever = vectordb_bge.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our vector store and retrieval system ready. We then need a large language model (LLM) to process information and answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open-source LLMs from Hugging Face**\n",
    "\n",
    "There are two ways to utilize Hugging Face LLMs: online and local.\n",
    "\n",
    "Hugging Face Hub\n",
    "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n",
    "\n",
    "To use, we should have the huggingface_hub python package installed.\n",
    "Set an environment variable called HUGGINGFACEHUB_API_TOKEN with your Hugging Face access token in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Samuel_Learnings\\NewsResearch_chatbot_Langchain\\sam_vs\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1, \"max_length\":500})\n",
    "hf.client.api_url = 'https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "query = \"Who is laila rizvi?\"  # Sample question, change to other questions you are interested in.\n",
    "#hf.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Hub will be slow when you run large models. You can get around this by downloading the model and run it on your local machine. This is the way we use LLM in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hugging Face Local Pipelines**\n",
    "\n",
    "Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "\n",
    "We need to install transformers python package.\n",
    "The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama-2–13B on all benchmarks tested. Read the paper.\n",
    "Mistral-7B-v0.1’s model size is 3.5GB, while Llama-2–13B has 13 billion parameters and 25GB model size.\n",
    "In order to use Llama2, you need to request access from Meta. Mistral-7B-v0.1 is publicly available already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "llm = hf \n",
    "query = \"Who is laila rizvi?\"\n",
    "#llm.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q & A chain**\n",
    "\n",
    "Now we have both the retrieval system for relevant documents and mistral AI LLM as QA chatbot ready.\n",
    "\n",
    "We will take our initial query, together with the relevant documents retrieved based on the results of our similarity search, to create a prompt to feed into the LLM. The LLM will take the initial query as the question and relevant documents as the context information to generate a result.\n",
    "\n",
    "LangChain provides an abstraction of the whole pipeline — RetrievalQA\n",
    "\n",
    "Let’s first construct a proper prompt for our task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling LangChain’s RetrievalQA with the prompt above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use RetrievalQA invoke method to execute the chain**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "#print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Laila Rizvi is a software engineer at Meta. She has a degree in computer science from the University of California, Berkeley. She has also worked as a software engineer at Google and Facebook.\n",
      "\n",
      "Laila Rizvi is a software engineer at Meta. She has a degree in computer science from the University of California, Berkeley. She has also worked as a software engineer at Google and Facebook.\n",
      "\n",
      "Laila Rizvi is a software engineer at Meta.\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the QA chain with our query.\n",
    "result2 = retrievalQA.invoke({\"query\": 'what laila rizvi said??'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lila Rizvi is a writer and editor who has worked in the publishing industry for over 10 years. She has a degree in English literature from the University of California, Berkeley, and has written for a variety of publications, including The New York Times, The Atlantic, and The Guardian.\n",
      "\n",
      "Rizvi is the author of the book \"The New York Times Book of the Dead,\" which is a collection of essays about the deaths of famous people. She has also\n"
     ]
    }
   ],
   "source": [
    "print(result2['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_vs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
